/gpfs/commons/home/jsingh/.conda/envs/spatial/lib/python3.9/site-packages/distributed/cli/dask_worker.py:334: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  warnings.warn(
2022-07-03 15:48:41,673 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.4.201.91:42903'
2022-07-03 15:48:41,736 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.4.201.91:36089'
2022-07-03 15:48:41,737 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.4.201.91:42479'
2022-07-03 15:48:42,671 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.4.201.91:35735'
/gpfs/commons/home/jsingh/.conda/envs/spatial/lib/python3.9/contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/gpfs/commons/home/jsingh/.conda/envs/spatial/lib/python3.9/contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
/gpfs/commons/home/jsingh/.conda/envs/spatial/lib/python3.9/contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
2022-07-03 15:48:59,405 - distributed.diskutils - INFO - Found stale lock file and directory '/gpfs/commons/home/jsingh/NYGC-PySeq2500-Pipeline/src/post/dask-worker-space/worker-7gvu5px_', purging
2022-07-03 15:48:59,629 - distributed.diskutils - INFO - Found stale lock file and directory '/gpfs/commons/home/jsingh/NYGC-PySeq2500-Pipeline/src/post/dask-worker-space/worker-bxua5f3i', purging
2022-07-03 15:48:59,635 - distributed.diskutils - INFO - Found stale lock file and directory '/gpfs/commons/home/jsingh/NYGC-PySeq2500-Pipeline/src/post/dask-worker-space/worker-qoh5vhng', purging
2022-07-03 15:49:01,629 - distributed.diskutils - INFO - Found stale lock file and directory '/gpfs/commons/home/jsingh/NYGC-PySeq2500-Pipeline/src/post/dask-worker-space/worker-t6wdco8t', purging
/gpfs/commons/home/jsingh/.conda/envs/spatial/lib/python3.9/contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
2022-07-03 15:49:41,662 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.4.201.91:42903'.
2022-07-03 15:49:41,663 - distributed.nanny - INFO - Nanny asking worker to close
2022-07-03 15:49:41,664 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.4.201.91:36089'.
2022-07-03 15:49:41,664 - distributed.nanny - INFO - Nanny asking worker to close
2022-07-03 15:49:41,665 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.4.201.91:35735'.
2022-07-03 15:49:41,665 - distributed.nanny - INFO - Nanny asking worker to close
2022-07-03 15:49:41,665 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.4.201.91:42479'.
2022-07-03 15:49:41,666 - distributed.nanny - INFO - Nanny asking worker to close
2022-07-03 15:49:45,690 - distributed.nanny - WARNING - Worker process still alive after 3.99998893737793 seconds, killing
2022-07-03 15:49:45,690 - distributed.nanny - WARNING - Worker process still alive after 3.9999988555908206 seconds, killing
2022-07-03 15:49:45,691 - distributed.nanny - WARNING - Worker process still alive after 3.999999237060547 seconds, killing
2022-07-03 15:49:45,691 - distributed.nanny - WARNING - Worker process still alive after 3.999999237060547 seconds, killing
2022-07-03 15:49:45,696 - distributed.dask_worker - INFO - Timed out starting worker
2022-07-03 15:49:45,910 - distributed.dask_worker - INFO - End worker
2022-07-03 15:49:45,911 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29588 parent=29470 started daemon>
2022-07-03 15:49:45,911 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29584 parent=29470 started daemon>
2022-07-03 15:49:45,911 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29582 parent=29470 started daemon>
2022-07-03 15:49:45,911 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=29579 parent=29470 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x55ef89d47a30)

Current thread 0x00007f3dbb2f0740 (most recent call first):
<no Python frame>
/var/spool/slurmd/job24706428/slurm_script: line 12: 29470 Aborted                 /gpfs/commons/home/jsingh/.conda/envs/spatial/bin/python -m distributed.cli.dask_worker tcp://10.4.200.50:41695 --nthreads 3 --nprocs 4 --memory-limit 29.80GiB --name SLURMCluster-0 --nanny --death-timeout 60 --protocol tcp://
