/gpfs/commons/home/jsingh/.conda/envs/spatial/lib/python3.9/site-packages/distributed/cli/dask_worker.py:334: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  warnings.warn(
2022-07-03 15:50:24,361 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.4.201.92:44972'
2022-07-03 15:50:24,390 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.4.201.92:42286'
2022-07-03 15:50:24,399 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.4.201.92:45828'
2022-07-03 15:50:24,400 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.4.201.92:36602'
2022-07-03 15:50:25,736 - distributed.diskutils - INFO - Found stale lock file and directory '/gpfs/commons/home/jsingh/NYGC-PySeq2500-Pipeline/src/post/dask-worker-space/worker-cbwyozah', purging
2022-07-03 15:50:25,817 - distributed.diskutils - INFO - Found stale lock file and directory '/gpfs/commons/home/jsingh/NYGC-PySeq2500-Pipeline/src/post/dask-worker-space/worker-jxne42xx', purging
2022-07-03 15:50:25,828 - distributed.diskutils - INFO - Found stale lock file and directory '/gpfs/commons/home/jsingh/NYGC-PySeq2500-Pipeline/src/post/dask-worker-space/worker-i650_apx', purging
2022-07-03 15:50:26,183 - distributed.diskutils - INFO - Found stale lock file and directory '/gpfs/commons/home/jsingh/NYGC-PySeq2500-Pipeline/src/post/dask-worker-space/worker-ylvmqn4h', purging
2022-07-03 15:51:24,351 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.4.201.92:44972'.
2022-07-03 15:51:24,352 - distributed.nanny - INFO - Nanny asking worker to close
2022-07-03 15:51:24,353 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.4.201.92:42286'.
2022-07-03 15:51:24,353 - distributed.nanny - INFO - Nanny asking worker to close
2022-07-03 15:51:24,353 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.4.201.92:45828'.
2022-07-03 15:51:24,353 - distributed.nanny - INFO - Nanny asking worker to close
2022-07-03 15:51:24,354 - distributed.nanny - INFO - Closing Nanny at 'tcp://10.4.201.92:36602'.
2022-07-03 15:51:24,354 - distributed.nanny - INFO - Nanny asking worker to close
2022-07-03 15:51:28,377 - distributed.nanny - WARNING - Worker process still alive after 3.999995613098145 seconds, killing
2022-07-03 15:51:28,377 - distributed.nanny - WARNING - Worker process still alive after 3.999999237060547 seconds, killing
2022-07-03 15:51:28,377 - distributed.nanny - WARNING - Worker process still alive after 3.999999237060547 seconds, killing
2022-07-03 15:51:28,378 - distributed.nanny - WARNING - Worker process still alive after 3.999999237060547 seconds, killing
2022-07-03 15:51:28,381 - distributed.dask_worker - INFO - Timed out starting worker
2022-07-03 15:51:28,381 - distributed.dask_worker - INFO - End worker
2022-07-03 15:51:28,382 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36587 parent=35916 started daemon>
2022-07-03 15:51:28,382 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36585 parent=35916 started daemon>
2022-07-03 15:51:28,382 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36581 parent=35916 started daemon>
2022-07-03 15:51:28,382 - distributed.process - INFO - reaping stray process <SpawnProcess name='Dask Worker process (from Nanny)' pid=36578 parent=35916 started daemon>
Exception in thread AsyncProcess Dask Worker process (from Nanny) watch process join:
Traceback (most recent call last):
Fatal Python error: _enter_buffered_busy: could not acquire lock for <_io.BufferedWriter name='<stderr>'> at interpreter shutdown, possibly due to daemon threads
Python runtime state: finalizing (tstate=0x55f68b677a30)

Current thread 0x00007f35ec25e740 (most recent call first):
<no Python frame>
/var/spool/slurmd/job24706429/slurm_script: line 12: 35916 Aborted                 /gpfs/commons/home/jsingh/.conda/envs/spatial/bin/python -m distributed.cli.dask_worker tcp://10.4.200.50:41695 --nthreads 3 --nprocs 4 --memory-limit 29.80GiB --name SLURMCluster-1 --nanny --death-timeout 60 --protocol tcp://
