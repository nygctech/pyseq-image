/gpfs/commons/home/jsingh/.conda/envs/spatial/lib/python3.9/site-packages/distributed/cli/dask_worker.py:334: FutureWarning: The --nprocs flag will be removed in a future release. It has been renamed to --nworkers.
  warnings.warn(
2022-06-30 13:33:21,685 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.4.201.11:38148'
2022-06-30 13:33:21,729 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.4.201.11:39760'
2022-06-30 13:33:21,732 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.4.201.11:45887'
2022-06-30 13:33:21,754 - distributed.nanny - INFO -         Start Nanny at: 'tcp://10.4.201.11:45131'
2022-06-30 13:33:23,523 - distributed.diskutils - INFO - Found stale lock file and directory '/gpfs/commons/home/jsingh/NYGC-PySeq2500-Pipeline/src/post/dask-worker-space/worker-lydipnn7', purging
2022-06-30 13:33:26,399 - distributed.diskutils - INFO - Found stale lock file and directory '/gpfs/commons/home/jsingh/NYGC-PySeq2500-Pipeline/src/post/dask-worker-space/worker-z4h1g3x1', purging
2022-06-30 13:33:26,663 - distributed.diskutils - INFO - Found stale lock file and directory '/gpfs/commons/home/jsingh/NYGC-PySeq2500-Pipeline/src/post/dask-worker-space/worker-0u5o8rkg', purging
2022-06-30 13:33:26,695 - distributed.diskutils - INFO - Found stale lock file and directory '/gpfs/commons/home/jsingh/NYGC-PySeq2500-Pipeline/src/post/dask-worker-space/worker-f24vkvrl', purging
/gpfs/commons/home/jsingh/.conda/envs/spatial/lib/python3.9/contextlib.py:126: UserWarning: Creating scratch directories is taking a surprisingly long time. This is often due to running workers on a network file system. Consider specifying a local-directory to point workers to write scratch data to a local disk.
  next(self.gen)
2022-06-30 13:33:32,741 - distributed.worker - INFO -       Start worker at:    tcp://10.4.201.11:39468
2022-06-30 13:33:32,741 - distributed.worker - INFO -       Start worker at:    tcp://10.4.201.11:39074
2022-06-30 13:33:32,742 - distributed.worker - INFO -          Listening to:    tcp://10.4.201.11:39468
2022-06-30 13:33:32,742 - distributed.worker - INFO -          Listening to:    tcp://10.4.201.11:39074
2022-06-30 13:33:32,742 - distributed.worker - INFO -          dashboard at:          10.4.201.11:36273
2022-06-30 13:33:32,742 - distributed.worker - INFO -       Start worker at:    tcp://10.4.201.11:35063
2022-06-30 13:33:32,742 - distributed.worker - INFO -          dashboard at:          10.4.201.11:33844
2022-06-30 13:33:32,742 - distributed.worker - INFO - Waiting to connect to:    tcp://10.4.200.39:38499
2022-06-30 13:33:32,742 - distributed.worker - INFO -          Listening to:    tcp://10.4.201.11:35063
2022-06-30 13:33:32,742 - distributed.worker - INFO - Waiting to connect to:    tcp://10.4.200.39:38499
2022-06-30 13:33:32,742 - distributed.worker - INFO - -------------------------------------------------
2022-06-30 13:33:32,742 - distributed.worker - INFO - -------------------------------------------------
2022-06-30 13:33:32,742 - distributed.worker - INFO -          dashboard at:          10.4.201.11:44433
2022-06-30 13:33:32,742 - distributed.worker - INFO -               Threads:                          3
2022-06-30 13:33:32,742 - distributed.worker - INFO - Waiting to connect to:    tcp://10.4.200.39:38499
2022-06-30 13:33:32,742 - distributed.worker - INFO -               Threads:                          3
2022-06-30 13:33:32,742 - distributed.worker - INFO - -------------------------------------------------
2022-06-30 13:33:32,742 - distributed.worker - INFO -                Memory:                  29.80 GiB
2022-06-30 13:33:32,742 - distributed.worker - INFO -                Memory:                  29.80 GiB
2022-06-30 13:33:32,742 - distributed.worker - INFO -       Local Directory: /gpfs/commons/home/jsingh/NYGC-PySeq2500-Pipeline/src/post/dask-worker-space/worker-7gvu5px_
2022-06-30 13:33:32,742 - distributed.worker - INFO -       Local Directory: /gpfs/commons/home/jsingh/NYGC-PySeq2500-Pipeline/src/post/dask-worker-space/worker-bxua5f3i
2022-06-30 13:33:32,742 - distributed.worker - INFO -               Threads:                          3
2022-06-30 13:33:32,742 - distributed.worker - INFO - -------------------------------------------------
2022-06-30 13:33:32,744 - distributed.worker - INFO -       Start worker at:    tcp://10.4.201.11:33368
2022-06-30 13:33:32,747 - distributed.worker - INFO - -------------------------------------------------
2022-06-30 13:33:32,747 - distributed.worker - INFO -                Memory:                  29.80 GiB
2022-06-30 13:33:32,747 - distributed.worker - INFO -          Listening to:    tcp://10.4.201.11:33368
2022-06-30 13:33:32,747 - distributed.worker - INFO -       Local Directory: /gpfs/commons/home/jsingh/NYGC-PySeq2500-Pipeline/src/post/dask-worker-space/worker-qoh5vhng
2022-06-30 13:33:32,747 - distributed.worker - INFO -          dashboard at:          10.4.201.11:36641
2022-06-30 13:33:32,747 - distributed.worker - INFO - -------------------------------------------------
2022-06-30 13:33:32,747 - distributed.worker - INFO - Waiting to connect to:    tcp://10.4.200.39:38499
2022-06-30 13:33:32,747 - distributed.worker - INFO - -------------------------------------------------
2022-06-30 13:33:32,747 - distributed.worker - INFO -               Threads:                          3
2022-06-30 13:33:32,747 - distributed.worker - INFO -                Memory:                  29.80 GiB
2022-06-30 13:33:32,747 - distributed.worker - INFO -       Local Directory: /gpfs/commons/home/jsingh/NYGC-PySeq2500-Pipeline/src/post/dask-worker-space/worker-t6wdco8t
2022-06-30 13:33:32,747 - distributed.worker - INFO - -------------------------------------------------
2022-06-30 13:33:32,777 - distributed.worker - INFO -         Registered to:    tcp://10.4.200.39:38499
2022-06-30 13:33:32,777 - distributed.worker - INFO - -------------------------------------------------
2022-06-30 13:33:32,778 - distributed.core - INFO - Starting established connection
2022-06-30 13:33:32,778 - distributed.worker - INFO -         Registered to:    tcp://10.4.200.39:38499
2022-06-30 13:33:32,778 - distributed.worker - INFO - -------------------------------------------------
2022-06-30 13:33:32,779 - distributed.core - INFO - Starting established connection
2022-06-30 13:33:32,781 - distributed.worker - INFO -         Registered to:    tcp://10.4.200.39:38499
2022-06-30 13:33:32,781 - distributed.worker - INFO - -------------------------------------------------
2022-06-30 13:33:32,781 - distributed.core - INFO - Starting established connection
2022-06-30 13:33:32,782 - distributed.worker - INFO -         Registered to:    tcp://10.4.200.39:38499
2022-06-30 13:33:32,782 - distributed.worker - INFO - -------------------------------------------------
2022-06-30 13:33:32,783 - distributed.core - INFO - Starting established connection
2022-06-30 13:33:36,976 - distributed.core - INFO - Event loop was unresponsive in Worker for 4.20s.  This is often caused by long-running GIL-holding functions or moving large chunks of data. This can cause timeouts and instability.
2022-06-30 13:34:02,547 - distributed.worker - WARNING - Compute Failed
Key:       get_pixels-e96d63d975de1873b9866fe3a57e3b83
Function:  get_pixels
args:      (range(0, 5))
kwargs:    {}
Exception: 'TypeError("unsupported operand type(s) for +: \'range\' and \'int\'")'

slurmstepd: error: *** JOB 24573229 ON pe2cc2-001 CANCELLED AT 2022-06-30T14:33:21 DUE TO TIME LIMIT ***
